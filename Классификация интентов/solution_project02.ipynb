{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fbfad8e-5ff1-4d40-aab5-05323edb71fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 01:45:12.470151: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "# Adam с исправлениями и планировщик learning rate\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1004b637-437b-4017-a1ee-ab8bf3eed20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "477c3f50-b0e1-4d7b-9e8e-dc33171bba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "DATA_PATH = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c78a20c7-4590-49b6-bf4a-cb8d82090c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>Кто написал Отверженных?</td>\n",
       "      <td>ODQA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Какие рецепты ты знаешь?</td>\n",
       "      <td>COOKING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>С тобой весело</td>\n",
       "      <td>LEGEND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>Как добраться до ашана? . Сколько это займет п...</td>\n",
       "      <td>NAVIGATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>Зачем Ван Гог отрезал себе ухо?</td>\n",
       "      <td>ODQA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text     Label\n",
       "930                          Кто написал Отверженных?       ODQA\n",
       "88                            Какие рецепты ты знаешь?   COOKING\n",
       "97                                      С тобой весело    LEGEND\n",
       "348  Как добраться до ашана? . Сколько это займет п...  NAVIGATE\n",
       "617                    Зачем Ван Гог отрезал себе ухо?      ODQA"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load train and validation data\n",
    "train = pd.read_csv(os.path.join(DATA_PATH, \"project02_not_full_train.csv\"))\n",
    "train, valid = train_test_split(train, stratify=train['Label'], random_state=RANDOM_STATE, train_size=0.85, shuffle=True)\n",
    "\n",
    "# test = pd.read_csv(os.path.join(DATA_PATH, \"project02_toloka_unlabeled.csv\"))\n",
    "test = pd.read_csv(os.path.join(DATA_PATH, \"project02_submission_file.csv\"))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502381f-92c9-4be4-a779-d750697ddd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30ca2532-da5a-4485-b6b8-77bd096697f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2b88c93add4c53b88f89b5cbded14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb8eb2e670f4584b1ecb541971b9909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b8c529e01f459ba1386bc2bb1adeae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0a94a67f2d443ab7e1f364775d9cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "974/172 - train/validation split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/envs/dl38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "\n",
    "def convert_comments_to_tensors(comments):\n",
    "    features = []\n",
    "    for comment in comments:\n",
    "        # full preparation for input to BERT model, including BPE-encoding,\n",
    "        # converting tokens to ids, padding, adding special tokens in the beginning and end of a sequence \n",
    "        items = tokenizer.encode_plus(\n",
    "            comment, \n",
    "            max_length=100, \n",
    "            truncation=True, \n",
    "            add_special_tokens=True, \n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "        features.append(items)\n",
    "\n",
    "    input_ids = torch.tensor([f['input_ids'] for f in features], dtype=torch.long)\n",
    "    # a mask, it has 1 - where a token exists and 0 where it's a padding index\n",
    "    attention_mask = torch.tensor([f['attention_mask'] for f in features], dtype=torch.long)\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "\n",
    "x_train = convert_comments_to_tensors(train['Text'].values)\n",
    "x_val = convert_comments_to_tensors(valid['Text'].values)\n",
    "\n",
    "mlb = LabelEncoder()\n",
    "y_train = mlb.fit_transform(train['Label'].values)\n",
    "y_val = mlb.transform(valid['Label'].values)\n",
    "\n",
    "print(\"{}/{} - train/validation split\".format(y_train.shape[0], y_val.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d0b6195-b551-43b7-b8a1-34b2679e8d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   101,  26154,  12715, 115950,   4346,    166,    102,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c62f8b-e85f-4443-bfdf-1f0cafc24f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13d483b0-eeef-45d9-832b-b9d7010ac0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c512f318f433411f979683b9daffe8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/681M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "config = BertConfig.from_pretrained(\n",
    "    'DeepPavlov/rubert-base-cased', # bert-base-uncased\n",
    "    num_labels=len(mlb.classes_)\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'DeepPavlov/rubert-base-cased',\n",
    "    from_tf=False,\n",
    "    config=config\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7730b56-f310-45a9-a9b9-a4dcc6b9d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    total_loss = 0.\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    model.eval()  # Set mode to evaluation to disable dropout & freeze BN\n",
    "    data_loader = DataLoader(data, batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(data_loader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {\n",
    "                'input_ids': batch[0],\n",
    "                'attention_mask': batch[1],\n",
    "                'labels': batch[2]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            total_loss += outputs[0]\n",
    "            y_pred.extend(outputs[1].cpu().numpy())\n",
    "            y_true.extend(batch[2].cpu().numpy())\n",
    "\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    y_true = np.asarray(y_true)\n",
    "\n",
    "    f1 = skm.f1_score(y_true, y_pred.argmax(1), average='macro')\n",
    "\n",
    "    return {'val_f1': f1, 'val_loss': total_loss / len(data)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee00b866-9783-46ca-87f2-37c4db97d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Identify whether metric has not been improved for certain number of epochs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 mode: str = 'min',\n",
    "                 min_delta: float = 0,\n",
    "                 patience: int = 10):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "\n",
    "        self.is_better = None\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda *_: True\n",
    "        else:\n",
    "            self._init_is_better(mode, min_delta)\n",
    "\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def step(self, current) -> bool:\n",
    "        \"\"\"\n",
    "        Make decision whether to stop training\n",
    "\n",
    "        :param current: new metric value\n",
    "        :return: whether to stop\n",
    "        \"\"\"\n",
    "        if isinstance(current, torch.Tensor):\n",
    "            current = current.cpu()\n",
    "        if np.isnan(current):\n",
    "            return True\n",
    "\n",
    "        if self.best is None:\n",
    "            self.best = current\n",
    "        else:\n",
    "            if self.is_better(current, self.best):\n",
    "                self.num_bad_epochs = 0\n",
    "                self.best = current\n",
    "            else:\n",
    "                self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if mode == 'min':\n",
    "            self.is_better = lambda value, best: value < best - min_delta\n",
    "        if mode == 'max':\n",
    "            self.is_better = lambda value, best: value > best + min_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e6e1821-de99-4ef8-a5c4-b34a2e517004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpoint:\n",
    "    \"\"\"Save the model after every epoch.\n",
    "    `filepath` can contain named formatting options,\n",
    "    which will be filled the value of `epoch` and `val_loss`.\n",
    "    For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`,\n",
    "    then the model checkpoints will be saved with the epoch number and\n",
    "    the validation loss in the filename.\n",
    "    # Arguments\n",
    "        model: PyTorch model object\n",
    "        filepath: string, path to save the model file.\n",
    "        save_best_only: if `save_best_only=True`,\n",
    "            the latest best model according to\n",
    "            the quantity monitored will not be overwritten.\n",
    "        mode: one of {min, max}.\n",
    "            If `save_best_only=True`, the decision\n",
    "            to overwrite the current save file is made\n",
    "            based on either the maximization or the\n",
    "            minimization of the monitored quantity. For `val_acc`,\n",
    "            this should be `max`, for `val_loss` this should\n",
    "            be `min`, etc.\n",
    "        save_weights_only: if True, then only the model's weights will be\n",
    "            saved, else the full model is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        filepath: str,\n",
    "        mode: str = \"min\",\n",
    "        save_best_only: bool = True,\n",
    "        save_weights_only: bool = False,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.filepath = filepath\n",
    "        self.mode = mode\n",
    "        self.save_best_only = save_best_only\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.num_saves = 0\n",
    "\n",
    "        if mode == \"min\":\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == \"max\":\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            raise ValueError(\"mode \" + mode + \" is unknown!\")\n",
    "\n",
    "        Path(self.filepath).parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    def _save_model(self):\n",
    "        if self.save_weights_only:\n",
    "            torch.save(self.model.state_dict(), self.filepath)\n",
    "        else:\n",
    "            torch.save(self.model, self.filepath)\n",
    "        self.num_saves += 1\n",
    "\n",
    "    def step(self, current, epoch=None):\n",
    "        if isinstance(current, torch.Tensor):\n",
    "            current = current.cpu()\n",
    "        if self.save_best_only:\n",
    "            if self.monitor_op(current, self.best):\n",
    "                self.best = current\n",
    "                self._save_model()\n",
    "        else:\n",
    "            self._save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6d291ad-e012-45aa-8920-0323413ace96",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0000125  # usually from 1e-5 until 8e-5\n",
    "warmup_steps = 50\n",
    "num_steps = 12000\n",
    "\n",
    "optimizer = AdamW([p for p in model.parameters() if p.requires_grad],\n",
    "                   lr=lr, weight_decay=0)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_steps)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=8, mode='max')\n",
    "model_checkpoint = ModelCheckpoint(model, 'models/rubert_base_cased_model.pt', mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c20eee52-1c73-4b85-88da-813d5ae56be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of epochs: 201\n",
      "Step 100, val_f1: 0.1858 val_loss: 0.1448 train_loss: 2.9354\n",
      "Step 200, val_f1: 0.3852 val_loss: 0.0916 train_loss: 1.8113\n",
      "Step 300, val_f1: 0.5372 val_loss: 0.0609 train_loss: 1.0425\n",
      "Step 400, val_f1: 0.6606 val_loss: 0.0473 train_loss: 0.5741\n",
      "Step 500, val_f1: 0.7028 val_loss: 0.0447 train_loss: 0.3265\n",
      "Step 600, val_f1: 0.8386 val_loss: 0.0358 train_loss: 0.1666\n",
      "Step 700, val_f1: 0.7399 val_loss: 0.0398 train_loss: 0.0886\n",
      "Step 800, val_f1: 0.8382 val_loss: 0.0410 train_loss: 0.0514\n",
      "Step 900, val_f1: 0.8462 val_loss: 0.0388 train_loss: 0.0321\n",
      "Step 1000, val_f1: 0.8550 val_loss: 0.0388 train_loss: 0.0259\n",
      "Step 1100, val_f1: 0.8460 val_loss: 0.0411 train_loss: 0.0186\n",
      "Step 1200, val_f1: 0.8089 val_loss: 0.0399 train_loss: 0.0164\n",
      "Step 1300, val_f1: 0.8460 val_loss: 0.0419 train_loss: 0.0120\n",
      "Step 1400, val_f1: 0.8573 val_loss: 0.0417 train_loss: 0.0104\n",
      "Step 1500, val_f1: 0.9008 val_loss: 0.0428 train_loss: 0.0085\n",
      "Step 1600, val_f1: 0.8957 val_loss: 0.0436 train_loss: 0.0077\n",
      "Step 1700, val_f1: 0.8474 val_loss: 0.0445 train_loss: 0.0067\n",
      "Step 1800, val_f1: 0.8988 val_loss: 0.0449 train_loss: 0.0062\n",
      "Step 1900, val_f1: 0.8568 val_loss: 0.0458 train_loss: 0.0053\n",
      "Step 2000, val_f1: 0.8554 val_loss: 0.0464 train_loss: 0.0049\n",
      "Step 2100, val_f1: 0.8620 val_loss: 0.0462 train_loss: 0.0045\n",
      "Step 2200, val_f1: 0.8643 val_loss: 0.0465 train_loss: 0.0041\n",
      "Step 2300, val_f1: 0.8629 val_loss: 0.0469 train_loss: 0.0037\n",
      "Early training stopping!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "gradient_accumulation_steps = 1\n",
    "logging_steps = 100  # периодичность проверки качества модели, чтобы во время остановить обучение\n",
    "max_grad_norm = 1\n",
    "\n",
    "# стандартный pytorch код для обертки входных данных и выходных классов в загрузчик данных\n",
    "train_dataset = TensorDataset(x_train[0], x_train[1], torch.LongTensor(y_train))\n",
    "val_dataset = TensorDataset(x_val[0], x_val[1], torch.LongTensor(y_val))\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "num_train_epochs = num_steps // (len(train_dataloader) // gradient_accumulation_steps) + 1\n",
    "global_step = 0\n",
    "tr_loss, logging_loss = 0.0, 0.0\n",
    "print('Count of epochs: %s' % num_train_epochs)\n",
    "\n",
    "for _ in range(num_train_epochs):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "        outputs = model(**inputs) # model outputs are tuple: (loss, logits)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            # Log metrics\n",
    "            if global_step % logging_steps == 0:\n",
    "                results = evaluate(val_dataset)\n",
    "                results.update({'train_loss': (tr_loss - logging_loss) / logging_steps})\n",
    "                print('Step {:3}, {}'.format(global_step, ' '.join(['{}: {:<6.4f}'.format(k, v) for k, v in\n",
    "                                                                  results.items()])))\n",
    "                logging_loss = tr_loss\n",
    "\n",
    "                # Saving model checkpoint here if we have improvement\n",
    "                model_checkpoint.step(results[\"val_f1\"])\n",
    "\n",
    "                if early_stopping.step(results['val_f1']):\n",
    "                    global_step = num_steps + 1\n",
    "                    print('Early training stopping!')\n",
    "                    break\n",
    "\n",
    "    if global_step > num_steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "246495a0-d97d-46d0-b0b7-3632b11b5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = convert_comments_to_tensors(test['Text'].values)\n",
    "test_dataset = TensorDataset(x_test[0], x_test[1])\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13ba31f7-42ac-497c-87cd-e77498a50cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "model.eval()  # Set mode to evaluation to disable dropout & freeze BN\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(test_data_loader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        y_pred.extend(mlb.inverse_transform(outputs[0].argmax(axis=1).to('cpu').numpy()))\n",
    "test['Label'] = np.asarray(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "967353db-ae98-42b3-bdd9-23539ee0a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('dmitry.ivashnikov_project02.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00b51396-e487-4e4a-bc07-25dc5496b75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 85.192.32.238:80...\n",
      "* Connected to de.newprolab.com (85.192.32.238) port 80 (#0)\n",
      "* Server auth using Basic with user 'upload'\n",
      "> PUT /upload/dmitry.ivashnikov_project02.csv HTTP/1.1\n",
      "> Host: de.newprolab.com\n",
      "> Authorization: Basic dXBsb2FkOm5ld3Byb2xhYnVwbG9hZA==\n",
      "> User-Agent: curl/7.71.1\n",
      "> Accept: */*\n",
      "> Content-Length: 20361\n",
      "> Expect: 100-continue\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 100 Continue\n",
      "* We are completely uploaded and fine\n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 204 No Content\n",
      "< Server: nginx/1.10.3 (Ubuntu)\n",
      "< Date: Mon, 06 Dec 2021 22:58:51 GMT\n",
      "< Connection: keep-alive\n",
      "< \n",
      "* Connection #0 to host de.newprolab.com left intact\n"
     ]
    }
   ],
   "source": [
    "!curl --user upload:newprolabupload -T dmitry.ivashnikov_project02.csv 'http://de.newprolab.com/upload/' -vvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ac782-ea8e-4399-9fee-dfb3e10f2492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cef045-fdd9-4e65-bd77-d898b3e7971a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl38",
   "language": "python",
   "name": "dl38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
