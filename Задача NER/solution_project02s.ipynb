{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d70e00b-3139-4982-8bd9-1966de7079a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 04:47:02.121478: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, BertTokenizerFast, BertTokenizer, BertForTokenClassification, BertConfig\n",
    "# Adam с исправлениями и планировщик learning rate\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "268380f0-b26e-432b-95ef-cab24d0342d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ac47cc0-74b4-4b38-9e72-cf20f0d6aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "DATA_PATH = 'data'\n",
    "DEVICE = 'cuda'\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4230bbaa-cdc7-4332-9af8-0c26e4c67b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>зайти так сейчас eos</td>\n",
       "      <td>O O O will_come</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>что сказать   а eos</td>\n",
       "      <td>B-dont_know_what_to_say I-dont_know_what_to_sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>а э   мат капитал можно использовать   а э eos</td>\n",
       "      <td>O O B-favorable_terms_and_offers I-favorable_t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>так а гнать eos</td>\n",
       "      <td>O O O no_aim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>девушка ничто не объяснить толком   и а eos</td>\n",
       "      <td>B-consult_better I-consult_better I-consult_be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Text  \\\n",
       "0                            зайти так сейчас eos   \n",
       "1                             что сказать   а eos   \n",
       "2  а э   мат капитал можно использовать   а э eos   \n",
       "3                                 так а гнать eos   \n",
       "4     девушка ничто не объяснить толком   и а eos   \n",
       "\n",
       "                                                Tags  \n",
       "0                                    O O O will_come  \n",
       "1  B-dont_know_what_to_say I-dont_know_what_to_sa...  \n",
       "2  O O B-favorable_terms_and_offers I-favorable_t...  \n",
       "3                                       O O O no_aim  \n",
       "4  B-consult_better I-consult_better I-consult_be...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load train and validation data\n",
    "train = pd.read_csv(os.path.join(DATA_PATH, \"data.train.csv\"))\n",
    "valid = pd.read_csv(os.path.join(DATA_PATH, \"data.valid.csv\"))\n",
    "test = pd.read_csv(os.path.join(DATA_PATH, \"data.test.csv\"))\n",
    "test = test.rename({'text': 'Text'}, axis=1)\n",
    "# train, valid = train_test_split(train, stratify=train['Label'], random_state=RANDOM_STATE, train_size=0.85, shuffle=True)\n",
    "\n",
    "# test = pd.read_csv(os.path.join(DATA_PATH, \"project02_toloka_unlabeled.csv\"))\n",
    "example = pd.read_csv(os.path.join(DATA_PATH, \"project02s_submission_file.csv\"))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a849b75-0f8b-4602-83ad-cd9ec8d1ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained('DeepPavlov/rubert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5670f5e7-824e-437a-af6e-3480429e9523",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_ids = {k: v for v, k in enumerate([tag for tag in train['Tags'].str.split().explode().unique()])}\n",
    "ids_to_labels = {v: k for k, v in labels_to_ids.items()}\n",
    "# ids_to_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1de9aef-982a-4869-af73-bc6724a4aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # step 1: get the sentence and word labels \n",
    "        sentence = self.data['Text'].iloc[index].strip().split()  \n",
    "\n",
    "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
    "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            is_split_into_words=True, \n",
    "            return_offsets_mapping=True, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_len\n",
    "        )\n",
    "\n",
    "        # step 4: turn everything into PyTorch tensors\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        \n",
    "        if 'Tags' in self.data.columns:\n",
    "            word_labels = self.data['Tags'].iloc[index].split()  \n",
    "            # step 3: create token labels only for first word pieces of each tokenized word\n",
    "            labels = [labels_to_ids[label] for label in word_labels] \n",
    "            # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
    "            # create an empty array of -100 of length max_length\n",
    "            encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "\n",
    "            # set only labels whose first offset position is 0 and the second is not 0\n",
    "            i = 0\n",
    "            for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "                if mapping[0] == 0 and mapping[1] != 0:\n",
    "                    # overwrite label\n",
    "                    encoded_labels[idx] = labels[i]\n",
    "                    i += 1        \n",
    "            item['labels'] = torch.as_tensor(encoded_labels)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e57b419-2ccf-4758-99ab-ac63c9a239a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=118, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(\n",
    "    'DeepPavlov/rubert-base-cased', # bert-base-uncased\n",
    "    num_labels=len(labels_to_ids)\n",
    ")\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    'DeepPavlov/rubert-base-cased',\n",
    "    from_tf=False,\n",
    "    num_labels=len(labels_to_ids)\n",
    ")\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a00b4d0-2e67-457d-bae4-5cf5d70fa42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    total_loss = 0.\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    model.eval()  # Set mode to evaluation to disable dropout & freeze BN\n",
    "    data_loader = DataLoader(data, batch_size=batch_size)\n",
    "    with torch.no_grad():           \n",
    "        with tqdm(data_loader, leave=False) as iterator:\n",
    "            for step, batch in enumerate(iterator):\n",
    "                # batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "                ids = batch['input_ids'].to(DEVICE, dtype = torch.long)\n",
    "                mask = batch['attention_mask'].to(DEVICE, dtype = torch.long)\n",
    "                labels = batch['labels'].to(DEVICE, dtype = torch.long)\n",
    "\n",
    "                inputs = {\n",
    "                    'input_ids': ids,\n",
    "                    'attention_mask': mask,\n",
    "                    'labels': labels\n",
    "                }\n",
    "\n",
    "                outputs = model(**inputs)\n",
    "                total_loss += outputs[0]\n",
    "                y_pred.extend(outputs[1].cpu().numpy().argmax(2))\n",
    "                y_true.extend(batch['labels'].numpy())\n",
    "\n",
    "                iterator.set_description(f\"\"\"Valid loss: {outputs[0].item():.4}\"\"\")\n",
    "\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "\n",
    "    mask = y_true != -100\n",
    "    f1 = skm.f1_score(y_true[mask], y_pred[mask], average='macro')\n",
    "\n",
    "    return {'val_f1': f1, 'val_loss': total_loss / len(data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce5cd7b2-94af-40bc-bc45-9729b0d819f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Identify whether metric has not been improved for certain number of epochs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 mode: str = 'min',\n",
    "                 min_delta: float = 0,\n",
    "                 patience: int = 10):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "\n",
    "        self.is_better = None\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda *_: True\n",
    "        else:\n",
    "            self._init_is_better(mode, min_delta)\n",
    "\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def step(self, current) -> bool:\n",
    "        \"\"\"\n",
    "        Make decision whether to stop training\n",
    "\n",
    "        :param current: new metric value\n",
    "        :return: whether to stop\n",
    "        \"\"\"\n",
    "        if isinstance(current, torch.Tensor):\n",
    "            current = current.cpu()\n",
    "        if np.isnan(current):\n",
    "            return True\n",
    "\n",
    "        if self.best is None:\n",
    "            self.best = current\n",
    "        else:\n",
    "            if self.is_better(current, self.best):\n",
    "                self.num_bad_epochs = 0\n",
    "                self.best = current\n",
    "            else:\n",
    "                self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if mode == 'min':\n",
    "            self.is_better = lambda value, best: value < best - min_delta\n",
    "        if mode == 'max':\n",
    "            self.is_better = lambda value, best: value > best + min_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39dfd5b0-b314-4be1-982f-e219ab43d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpoint:\n",
    "    \"\"\"Save the model after every epoch.\n",
    "    `filepath` can contain named formatting options,\n",
    "    which will be filled the value of `epoch` and `val_loss`.\n",
    "    For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`,\n",
    "    then the model checkpoints will be saved with the epoch number and\n",
    "    the validation loss in the filename.\n",
    "    # Arguments\n",
    "        model: PyTorch model object\n",
    "        filepath: string, path to save the model file.\n",
    "        save_best_only: if `save_best_only=True`,\n",
    "            the latest best model according to\n",
    "            the quantity monitored will not be overwritten.\n",
    "        mode: one of {min, max}.\n",
    "            If `save_best_only=True`, the decision\n",
    "            to overwrite the current save file is made\n",
    "            based on either the maximization or the\n",
    "            minimization of the monitored quantity. For `val_acc`,\n",
    "            this should be `max`, for `val_loss` this should\n",
    "            be `min`, etc.\n",
    "        save_weights_only: if True, then only the model's weights will be\n",
    "            saved, else the full model is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        filepath: str,\n",
    "        mode: str = \"min\",\n",
    "        save_best_only: bool = True,\n",
    "        save_weights_only: bool = False,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.filepath = filepath\n",
    "        self.mode = mode\n",
    "        self.save_best_only = save_best_only\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.num_saves = 0\n",
    "\n",
    "        if mode == \"min\":\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == \"max\":\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            raise ValueError(\"mode \" + mode + \" is unknown!\")\n",
    "\n",
    "        Path(self.filepath).parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    def _save_model(self):\n",
    "        if self.save_weights_only:\n",
    "            torch.save(self.model.state_dict(), self.filepath)\n",
    "        else:\n",
    "            torch.save(self.model, self.filepath)\n",
    "        self.num_saves += 1\n",
    "\n",
    "    def step(self, current, epoch=None):\n",
    "        if isinstance(current, torch.Tensor):\n",
    "            current = current.cpu()\n",
    "        if self.save_best_only:\n",
    "            if self.monitor_op(current, self.best):\n",
    "                self.best = current\n",
    "                self._save_model()\n",
    "        else:\n",
    "            self._save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08d64a-dece-4cf1-a90a-a609c2dbebaa",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e9dea4f-5810-4a99-ab6a-f7e64f51df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0000125  # usually from 1e-5 until 8e-5\n",
    "warmup_steps = 50\n",
    "num_steps = 12000\n",
    "\n",
    "optimizer = AdamW([p for p in model.parameters() if p.requires_grad],\n",
    "                   lr=lr, weight_decay=0)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_steps)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=8, mode='max')\n",
    "model_checkpoint = ModelCheckpoint(model, 'models/rubert_base_cased_model.pt', mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dee56e8-f85c-4db0-9a08-7f3aa45d4c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of epochs: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 250, val_f1: 0.1062 val_loss: 0.0589 train_loss: 1.7722\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500, val_f1: 0.2388 val_loss: 0.0380 train_loss: 0.7153\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 750, val_f1: 0.3051 val_loss: 0.0277 train_loss: 0.4210\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000, val_f1: 0.4927 val_loss: 0.0186 train_loss: 0.2332\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1250, val_f1: 0.6143 val_loss: 0.0134 train_loss: 0.1391\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1500, val_f1: 0.6662 val_loss: 0.0096 train_loss: 0.0836\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1750, val_f1: 0.7289 val_loss: 0.0067 train_loss: 0.0527\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000, val_f1: 0.7639 val_loss: 0.0052 train_loss: 0.0388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2250, val_f1: 0.8226 val_loss: 0.0034 train_loss: 0.0312\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2500, val_f1: 0.8574 val_loss: 0.0026 train_loss: 0.0248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2750, val_f1: 0.8950 val_loss: 0.0019 train_loss: 0.0202\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3000, val_f1: 0.9049 val_loss: 0.0015 train_loss: 0.0159\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3250, val_f1: 0.9112 val_loss: 0.0012 train_loss: 0.0128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3500, val_f1: 0.9284 val_loss: 0.0011 train_loss: 0.0116\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3750, val_f1: 0.9448 val_loss: 0.0009 train_loss: 0.0105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000, val_f1: 0.9423 val_loss: 0.0007 train_loss: 0.0093\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4250, val_f1: 0.9806 val_loss: 0.0006 train_loss: 0.0078\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4500, val_f1: 0.9805 val_loss: 0.0005 train_loss: 0.0070\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4750, val_f1: 0.9855 val_loss: 0.0005 train_loss: 0.0064\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5000, val_f1: 0.9870 val_loss: 0.0006 train_loss: 0.0061\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5250, val_f1: 0.9937 val_loss: 0.0004 train_loss: 0.0059\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5500, val_f1: 0.9934 val_loss: 0.0004 train_loss: 0.0056\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5750, val_f1: 0.9879 val_loss: 0.0004 train_loss: 0.0051\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6000, val_f1: 0.9924 val_loss: 0.0004 train_loss: 0.0053\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6250, val_f1: 0.9931 val_loss: 0.0004 train_loss: 0.0045\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6500, val_f1: 0.9924 val_loss: 0.0004 train_loss: 0.0041\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6750, val_f1: 0.9936 val_loss: 0.0003 train_loss: 0.0037\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7000, val_f1: 0.9942 val_loss: 0.0002 train_loss: 0.0043\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7250, val_f1: 0.9943 val_loss: 0.0002 train_loss: 0.0035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7500, val_f1: 0.9939 val_loss: 0.0002 train_loss: 0.0038\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7750, val_f1: 0.9936 val_loss: 0.0002 train_loss: 0.0030\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8000, val_f1: 0.9942 val_loss: 0.0002 train_loss: 0.0031\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8250, val_f1: 0.9940 val_loss: 0.0002 train_loss: 0.0030\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8500, val_f1: 0.9950 val_loss: 0.0002 train_loss: 0.0026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8750, val_f1: 0.9952 val_loss: 0.0002 train_loss: 0.0028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9000, val_f1: 0.9953 val_loss: 0.0002 train_loss: 0.0024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9250, val_f1: 0.9942 val_loss: 0.0002 train_loss: 0.0022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9500, val_f1: 0.9960 val_loss: 0.0002 train_loss: 0.0024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9750, val_f1: 0.9956 val_loss: 0.0002 train_loss: 0.0022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10000, val_f1: 0.9973 val_loss: 0.0001 train_loss: 0.0023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10250, val_f1: 0.9970 val_loss: 0.0001 train_loss: 0.0019\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10500, val_f1: 0.9972 val_loss: 0.0001 train_loss: 0.0018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10750, val_f1: 0.9973 val_loss: 0.0001 train_loss: 0.0018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11000, val_f1: 0.9974 val_loss: 0.0001 train_loss: 0.0017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11250, val_f1: 0.9973 val_loss: 0.0001 train_loss: 0.0021\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11500, val_f1: 0.9974 val_loss: 0.0001 train_loss: 0.0013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84534da57d4e42c98dfb5280db6f45c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11750, val_f1: 0.9972 val_loss: 0.0001 train_loss: 0.0017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12000, val_f1: 0.9972 val_loss: 0.0001 train_loss: 0.0015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12250, val_f1: 0.9972 val_loss: 0.0001 train_loss: 0.0012\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12500, val_f1: 0.9972 val_loss: 0.0001 train_loss: 0.0015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12750, val_f1: 0.9972 val_loss: 0.0001 train_loss: 0.0014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13000, val_f1: 0.9972 val_loss: 0.0001 train_loss: 0.0020\n",
      "Early training stopping!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "gradient_accumulation_steps = 1\n",
    "logging_steps = 250  # периодичность проверки качества модели, чтобы во время остановить обучение\n",
    "max_grad_norm = 1\n",
    "\n",
    "# # стандартный pytorch код для обертки входных данных и выходных классов в загрузчик данных\n",
    "# train_dataset = TensorDataset(x_train[0], x_train[1], torch.LongTensor(y_train))\n",
    "# val_dataset = TensorDataset(x_val[0], x_val[1], torch.LongTensor(y_val))\n",
    "# train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "train_dataset = dataset(train, tokenizer, MAX_LEN)\n",
    "val_dataset = dataset(valid, tokenizer, MAX_LEN)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "num_train_epochs = num_steps // (len(train_dataloader) // gradient_accumulation_steps) + 1\n",
    "global_step = 0\n",
    "tr_loss, logging_loss = 0.0, 0.0\n",
    "print('Count of epochs: %s' % num_train_epochs)\n",
    "\n",
    "for _ in range(num_train_epochs):\n",
    "    with tqdm(train_dataloader, leave=False) as iterator:\n",
    "        for step, batch in enumerate(iterator):\n",
    "            model.train()\n",
    "            # batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            ids = batch['input_ids'].to(DEVICE, dtype = torch.long)\n",
    "            mask = batch['attention_mask'].to(DEVICE, dtype = torch.long)\n",
    "            labels = batch['labels'].to(DEVICE, dtype = torch.long)\n",
    "\n",
    "            inputs = {\n",
    "                'input_ids': ids,\n",
    "                'attention_mask': mask,\n",
    "                'labels': labels\n",
    "            }\n",
    "            outputs = model(**inputs) # model outputs are tuple: (loss, logits)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            \n",
    "            iterator.set_description(f\"\"\"Train loss: {loss.item():.4}\"\"\")\n",
    "            \n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                # Log metrics\n",
    "                if global_step % logging_steps == 0:\n",
    "                    results = evaluate(val_dataset)\n",
    "                    results.update({'train_loss': (tr_loss - logging_loss) / logging_steps})\n",
    "                    print('Step {:3}, {}'.format(global_step, ' '.join(['{}: {:<6.4f}'.format(k, v) for k, v in\n",
    "                                                                      results.items()])))\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                    # Saving model checkpoint here if we have improvement\n",
    "                    model_checkpoint.step(results[\"val_f1\"])\n",
    "\n",
    "                    if early_stopping.step(results['val_f1']):\n",
    "                        global_step = num_steps + 1\n",
    "                        print('Early training stopping!')\n",
    "                        break\n",
    "\n",
    "        if global_step > num_steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae526663-5824-4e11-a022-291880e32e93",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b0021ff-c980-4367-9723-03a2e3be64ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = dataset(test, tokenizer, MAX_LEN)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "model.eval()  # Set mode to evaluation to disable dropout & freeze BN\n",
    "\n",
    "with torch.no_grad():\n",
    "    with tqdm(test_dataloader, leave=False) as iterator:\n",
    "        for batch in iterator:\n",
    "\n",
    "            ids = batch['input_ids'].to(DEVICE, dtype = torch.long)\n",
    "            mask = batch['attention_mask'].to(DEVICE, dtype = torch.long)\n",
    "            batch_offset_mapping = batch['offset_mapping'].numpy()\n",
    "\n",
    "            inputs = {\n",
    "                'input_ids': ids,\n",
    "                'attention_mask': mask,\n",
    "            }\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            batch_logits = outputs[0].cpu().numpy()\n",
    "\n",
    "            batch_y_pred = []\n",
    "            for logits, offset_mapping in zip(batch_logits, batch_offset_mapping):\n",
    "                flattened_predictions = logits.argmax(1)\n",
    "                token_predictions = list(map(ids_to_labels.get, flattened_predictions))\n",
    "                batch_y_pred.append(' '.join([\n",
    "                    token_pred \n",
    "                    for token_pred, mapping in zip(token_predictions, offset_mapping) \n",
    "                    if mapping[0] == 0 and mapping[1] != 0 # only predictions on first word pieces are important\n",
    "                ]))\n",
    "\n",
    "            y_pred.extend(batch_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8eec404-641c-4b5b-9cec-399c74c060c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Tags'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab0e8e02-0d30-47a8-bdab-282ffb1a1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('dmitry.ivashnikov_project02s.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "775fd455-1664-4508-a970-e2560df93576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "*   Trying 85.192.32.238:80...\n",
      "* Connected to de.newprolab.com (85.192.32.238) port 80 (#0)\n",
      "* Server auth using Basic with user 'upload'\n",
      "> PUT /upload/dmitry.ivashnikov_project02s.csv HTTP/1.1\n",
      "> Host: de.newprolab.com\n",
      "> Authorization: Basic dXBsb2FkOm5ld3Byb2xhYnVwbG9hZA==\n",
      "> User-Agent: curl/7.71.1\n",
      "> Accept: */*\n",
      "> Content-Length: 2930798\n",
      "> Expect: 100-continue\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 100 Continue\n",
      "* We are completely uploaded and fine\n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 204 No Content\n",
      "< Server: nginx/1.10.3 (Ubuntu)\n",
      "< Date: Thu, 09 Dec 2021 04:53:30 GMT\n",
      "< Connection: keep-alive\n",
      "< \n",
      "* Connection #0 to host de.newprolab.com left intact\n"
     ]
    }
   ],
   "source": [
    "!curl --user upload:newprolabupload -T dmitry.ivashnikov_project02s.csv 'http://de.newprolab.com/upload/' -vvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e98c4c-3bc7-4d00-9c93-c4a8d5bfd4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl38",
   "language": "python",
   "name": "dl38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
