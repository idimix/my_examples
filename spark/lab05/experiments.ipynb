{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "PYSPARK_SUBMIT_ARGS = \"\"\"--num-executors 3 pyspark-shell\"\"\"\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = PYSPARK_SUBMIT_ARGS\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ivashnikov\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"GMT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, when, col, expr, lower, concat, lit, create_map, dayofweek, hour, collect_list, udf\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.types import IntegerType, ArrayType\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# open and transform weblogs\n",
    "timestamp = expr(\"from_unixtime(visits.timestamp/1000)\")\n",
    "\n",
    "hour_col = hour('timestamp').alias('hour')\n",
    "dayofweek_col = dayofweek('timestamp').alias('dayofweek')\n",
    "\n",
    "work_hours = when((9 <= col('hour')) & (col('hour') < 18), 1).otherwise(0).alias('web_fraction_work_hours')\n",
    "evening_hours = when((18 <= col('hour')) & (col('hour') < 24), 1).otherwise(0).alias('web_fraction_evening_hours')\n",
    "hours = [when(col('hour') == i, 1).otherwise(0).alias('web_hour_%s' % i) for i in range(24)]\n",
    "\n",
    "weekdays_dict = {1: 'sun', 2: 'mon', 3: 'tue', 4: 'wed', 5: 'thu', 6: 'fri', 7: 'sat'}\n",
    "days = [when(col('dayofweek') == k, 1).otherwise(0).alias('web_day_%s' % v) for k, v in weekdays_dict.items()]\n",
    "all_visits = lit(1).alias('all_visits')\n",
    "\n",
    "domain = expr(\"regexp_replace(parse_url(visits.url, 'HOST'), 'www.', '')\").alias('domain')\n",
    "\n",
    "weblogs = spark \\\n",
    "    .read.format('json') \\\n",
    "    .load('/mf-labs/laba02/weblogs.json') \\\n",
    "    .withColumn('visits', explode('visits')) \\\n",
    "    .withColumn('timestamp', timestamp) \\\n",
    "    .select('*', hour_col, dayofweek_col) \\\n",
    "    .select('uid', domain, all_visits, work_hours, evening_hours, *days, *hours)\n",
    "\n",
    "\n",
    "# create data_features\n",
    "data_features = weblogs \\\n",
    "    .drop('domain') \\\n",
    "    .groupby('uid') \\\n",
    "    .sum()\n",
    "\n",
    "for column in data_features.columns:\n",
    "    data_features = data_features.withColumnRenamed(column, re.sub('\\(|\\)|sum', '', column))\n",
    "\n",
    "data_features = data_features \\\n",
    "    .withColumn('web_fraction_work_hours', col('web_fraction_work_hours') / col('all_visits')) \\\n",
    "    .withColumn('web_fraction_evening_hours', col('web_fraction_evening_hours') / col('all_visits')) \\\n",
    "    .drop('all_visits')\n",
    "\n",
    "\n",
    "# create domain_features\n",
    "top1000 = weblogs \\\n",
    "    .where('domain != \"null\"') \\\n",
    "    .groupby('domain') \\\n",
    "    .count() \\\n",
    "    .sort(col('count').desc()) \\\n",
    "    .limit(1000) \\\n",
    "    .select('domain', lit(1).alias('index')) \\\n",
    "    .groupby('index') \\\n",
    "    .agg(collect_list(col('domain')).alias('domains'))\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"domains\", outputCol=\"domain_features\").fit(top1000)\n",
    "index_sorted = np.argsort(cv.vocabulary)\n",
    "vector_udf = udf(lambda vector: list(map(int, np.array(vector.toArray())[index_sorted])), ArrayType(IntegerType()))\n",
    "\n",
    "domain_features = weblogs \\\n",
    "        .groupby('uid') \\\n",
    "        .agg(collect_list(col('domain')).alias('domains'))\n",
    "domain_features = cv \\\n",
    "    .transform(domain_features) \\\n",
    "    .withColumn('domain_features', vector_udf('domain_features')) \\\n",
    "    .drop('domains')\n",
    "\n",
    "\n",
    "# open users-items and join all data\n",
    "users_items = spark \\\n",
    "    .read.format('parquet') \\\n",
    "    .load('/user/dmitry.ivashnikov/users-items/20200429')\n",
    "\n",
    "all_features = users_items.join(domain_features, on='uid').join(data_features, on='uid')\n",
    "\n",
    "# сохранение данных\n",
    "all_features \\\n",
    "    .write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save('/user/dmitry.ivashnikov/features')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
