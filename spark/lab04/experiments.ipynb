{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# create SparkSession\n",
    "##############################################################################\n",
    "\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "PYSPARK_SUBMIT_ARGS = \"\"\"--num-executors 3 pyspark-shell\"\"\"\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = PYSPARK_SUBMIT_ARGS\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ivashnikov\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **spark.users_items.update**: 0 или 1 режим работы, сделать новую матрицу users_items или добавить строки к существующей. По умолчанию - 1, добавить строки в матрицу.\n",
    "* **spark.users_items.output_dir**: абсолютный или относительный путь к выходным данным.\n",
    "* **spark.users_items.input_dir**: абсолютный или относительный путь к входным данным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "update = bool(int('0'))\n",
    "input_dir = '/user/dmitry.ivashnikov/visits'\n",
    "output_dir = '/user/dmitry.ivashnikov/users-items'\n",
    "\n",
    "\n",
    "# update = int(spark.conf.get(\"spark.users_items.update\"))\n",
    "# input_dir = spark.conf.get(\"spark.users_items.input_dir\")\n",
    "# output_dir = spark.conf.get(\"spark.users_items.output_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# вспомогательные функции\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, lower, concat, lit, max as spark_max, regexp_replace\n",
    "import re\n",
    "\n",
    "def read_df(action_type, input_dir=input_dir):\n",
    "    \"\"\"чтение датафрейма с заданным типом\"\"\"\n",
    "\n",
    "    item_id = concat(\n",
    "        lit(action_type), \n",
    "        lit('_'), \n",
    "        lower(regexp_replace(col('item_id'), '\\s|-', '_'))\n",
    "    ).alias('item_id')\n",
    "    \n",
    "    return spark \\\n",
    "        .read \\\n",
    "        .format('json') \\\n",
    "        .load(os.path.join(input_dir, action_type)) \\\n",
    "        .na.drop(subset='uid') \\\n",
    "        .select('date', 'uid', item_id)\n",
    "\n",
    "\n",
    "def union_df(df1, df2):\n",
    "    \"\"\"объединение двух датафреймов с разным набором колонок\"\"\"\n",
    "\n",
    "    columns1 = set(df1.columns)\n",
    "    columns2 = set(df2.columns)\n",
    "\n",
    "    columns1_new = columns2 - columns1\n",
    "    columns2_new = columns1 - columns2\n",
    "\n",
    "    expr1 = list(columns1) + list(map(lambda x: '0 as %s' % x, columns1_new))\n",
    "    expr2 = list(columns2) + list(map(lambda x: '0 as %s' % x, columns2_new))\n",
    "\n",
    "    df_union = df1.selectExpr(*expr1).unionByName(df2.selectExpr(*expr2))\n",
    "    \n",
    "    return df_union.select(sorted(df_union.columns))\n",
    "\n",
    "\n",
    "def group_sum_df(df, key):\n",
    "    \"\"\"группировка по ключу и сумма остальных колонок с последующим переименованием\"\"\"\n",
    "\n",
    "    df_new = df \\\n",
    "        .groupby(key) \\\n",
    "        .sum()\n",
    "\n",
    "    for col in df_new.columns:\n",
    "        df_new = df_new.withColumnRenamed(col, re.sub('\\(|\\)|sum', '', col))\n",
    "\n",
    "    return df_new\n",
    "\n",
    "\n",
    "def path_exists(path):\n",
    "    \"\"\"проверка, существует ли путь\"\"\"\n",
    "    \n",
    "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
    "    return fs.exists(sc._jvm.org.apache.hadoop.fs.Path(path))\n",
    "\n",
    "\n",
    "def get_file_list(path):\n",
    "    \"\"\"получение списка файлов в папке\"\"\"\n",
    "\n",
    "    hadoop = sc._jvm.org.apache.hadoop\n",
    "    fs = hadoop.fs.FileSystem\n",
    "    conf = hadoop.conf.Configuration()\n",
    "\n",
    "    path = hadoop.fs.Path(path)\n",
    "    file_list = [str(f.getPath()).rsplit('/', 1)[1] for f in fs.get(conf).listStatus(path) if not f.isDirectory()]\n",
    "\n",
    "    return file_list\n",
    "\n",
    "\n",
    "def get_dir_list(path):\n",
    "    \"\"\"получение списка файлов в папке\"\"\"\n",
    "\n",
    "    hadoop = sc._jvm.org.apache.hadoop\n",
    "    fs = hadoop.fs.FileSystem\n",
    "    conf = hadoop.conf.Configuration()\n",
    "\n",
    "    path = hadoop.fs.Path(path)\n",
    "    file_list = [str(f.getPath()).rsplit('/', 1)[1] for f in fs.get(conf).listStatus(path) if f.isDirectory()]\n",
    "\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = read_df('buy').union(read_df('view'))\n",
    "visits.cache()\n",
    "\n",
    "max_date_str = str(visits.agg(spark_max('date')).collect()[0][0])\n",
    "\n",
    "users_items_new = visits \\\n",
    "    .groupby('uid') \\\n",
    "    .pivot('item_id') \\\n",
    "    .count() \\\n",
    "    .na.fill(0)\n",
    "\n",
    "users_items_final = users_items_new\n",
    "\n",
    "if update:\n",
    "    cmd = 'hdfs dfs -ls %s' % os.path.join(output_dir)\n",
    "    pattern = ' (/.+)'\n",
    "    dir_list = os.popen(cmd).read().strip().split('\\n')\n",
    "    dir_list = list(filter(lambda x: re.search(pattern, x), dir_list))\n",
    "    dir_list = list(map(lambda x: re.search(pattern, x).group(1).rsplit('/', 1)[1], dir_list))\n",
    "    dir_list = list(filter(lambda x: x.isdigit(), dir_list))\n",
    "    last_dir = str(max(list(map(int, dir_list))))\n",
    "\n",
    "    # чтение последнего набора данных\n",
    "    users_items_old = spark \\\n",
    "        .read \\\n",
    "        .format(\"parquet\") \\\n",
    "        .load(os.path.join(output_dir, last_dir))\n",
    "\n",
    "    users_items_old.cache()\n",
    "    users_items_old.count()\n",
    "\n",
    "    # объединение новых и старых данных\n",
    "    users_items_final = group_sum_df(\n",
    "        union_df(users_items_new, users_items_old), 'uid'\n",
    "    )\n",
    "\n",
    "# сохранение данных\n",
    "users_items_final \\\n",
    "    .write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(os.path.join(output_dir, max_date_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
